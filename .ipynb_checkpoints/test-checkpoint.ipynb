{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        # First Convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        # Second Convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout2d(p=0.25)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)  # Adjusted input size after max pooling\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer with 10 classes (assuming it's a classification task)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Convolutional layer followed by ReLU activation\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # Second Convolutional layer followed by ReLU activation\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # Max pooling layer\n",
    "        x = self.pool(x)\n",
    "        # Dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(-1, 64 * 12 * 12)\n",
    "        # First fully connected layer followed by ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Dropout layer\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # Output layer with softmax activation\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)  # Applying softmax activation on output\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = MyCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Mini-batch   100] Loss: 2.017\n",
      "[Epoch 1, Mini-batch   200] Loss: 1.696\n",
      "[Epoch 1, Mini-batch   300] Loss: 1.653\n",
      "[Epoch 1, Mini-batch   400] Loss: 1.639\n",
      "[Epoch 1, Mini-batch   500] Loss: 1.637\n",
      "[Epoch 1, Mini-batch   600] Loss: 1.612\n",
      "[Epoch 1, Mini-batch   700] Loss: 1.607\n",
      "[Epoch 1, Mini-batch   800] Loss: 1.595\n",
      "[Epoch 1, Mini-batch   900] Loss: 1.582\n",
      "[Epoch 2, Mini-batch   100] Loss: 1.569\n",
      "[Epoch 2, Mini-batch   200] Loss: 1.564\n",
      "[Epoch 2, Mini-batch   300] Loss: 1.557\n",
      "[Epoch 2, Mini-batch   400] Loss: 1.544\n",
      "[Epoch 2, Mini-batch   500] Loss: 1.546\n",
      "[Epoch 2, Mini-batch   600] Loss: 1.547\n",
      "[Epoch 2, Mini-batch   700] Loss: 1.533\n",
      "[Epoch 2, Mini-batch   800] Loss: 1.543\n",
      "[Epoch 2, Mini-batch   900] Loss: 1.529\n",
      "[Epoch 3, Mini-batch   100] Loss: 1.521\n",
      "[Epoch 3, Mini-batch   200] Loss: 1.524\n",
      "[Epoch 3, Mini-batch   300] Loss: 1.523\n",
      "[Epoch 3, Mini-batch   400] Loss: 1.520\n",
      "[Epoch 3, Mini-batch   500] Loss: 1.517\n",
      "[Epoch 3, Mini-batch   600] Loss: 1.516\n",
      "[Epoch 3, Mini-batch   700] Loss: 1.516\n",
      "[Epoch 3, Mini-batch   800] Loss: 1.515\n",
      "[Epoch 3, Mini-batch   900] Loss: 1.509\n",
      "[Epoch 4, Mini-batch   100] Loss: 1.506\n",
      "[Epoch 4, Mini-batch   200] Loss: 1.505\n",
      "[Epoch 4, Mini-batch   300] Loss: 1.503\n",
      "[Epoch 4, Mini-batch   400] Loss: 1.505\n",
      "[Epoch 4, Mini-batch   500] Loss: 1.508\n",
      "[Epoch 4, Mini-batch   600] Loss: 1.507\n",
      "[Epoch 4, Mini-batch   700] Loss: 1.505\n",
      "[Epoch 4, Mini-batch   800] Loss: 1.500\n",
      "[Epoch 4, Mini-batch   900] Loss: 1.500\n",
      "[Epoch 5, Mini-batch   100] Loss: 1.499\n",
      "[Epoch 5, Mini-batch   200] Loss: 1.497\n",
      "[Epoch 5, Mini-batch   300] Loss: 1.501\n",
      "[Epoch 5, Mini-batch   400] Loss: 1.493\n",
      "[Epoch 5, Mini-batch   500] Loss: 1.501\n",
      "[Epoch 5, Mini-batch   600] Loss: 1.499\n",
      "[Epoch 5, Mini-batch   700] Loss: 1.499\n",
      "[Epoch 5, Mini-batch   800] Loss: 1.495\n",
      "[Epoch 5, Mini-batch   900] Loss: 1.491\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize the image data\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define your model\n",
    "model = MyCNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification tasks\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 100 == 0:    # Print every 100 mini-batches\n",
    "            print('[Epoch %d, Mini-batch %5d] Loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to my_cnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming your model is called `model` and is already trained\n",
    "\n",
    "# Define the path where you want to save the model\n",
    "model_save_path = 'my_cnn_model.pth'\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "\n",
    "# Define the CNN model (reuse your previously defined model)\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 64 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Load your trained model (make sure to set the model to eval mode)\n",
    "model = MyCNN()\n",
    "model.load_state_dict(torch.load('my_cnn_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create a simple drawing application using tkinter\n",
    "class DrawApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Draw a digit\")\n",
    "        self.canvas = tk.Canvas(root, width=200, height=200, bg='white')\n",
    "        self.canvas.pack()\n",
    "        self.canvas.bind(\"<B1-Motion>\", self.paint)\n",
    "        self.button_clear = tk.Button(root, text=\"Clear\", command=self.clear)\n",
    "        self.button_clear.pack()\n",
    "        self.button_predict = tk.Button(root, text=\"Predict\", command=self.predict)\n",
    "        self.button_predict.pack()\n",
    "        self.label = tk.Label(root, text=\"\", font=(\"Helvetica\", 24))\n",
    "        self.label.pack()\n",
    "        self.image = Image.new(\"L\", (200, 200), color=255)\n",
    "        self.draw = ImageDraw.Draw(self.image)\n",
    "\n",
    "    def paint(self, event):\n",
    "        x1, y1 = (event.x - 8), (event.y - 8)\n",
    "        x2, y2 = (event.x + 8), (event.y + 8)\n",
    "        self.canvas.create_oval(x1, y1, x2, y2, fill=\"black\", width=5)\n",
    "        self.draw.ellipse([x1, y1, x2, y2], fill=0)\n",
    "\n",
    "    def clear(self):\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.image = Image.new(\"L\", (200, 200), color=255)\n",
    "        self.draw = ImageDraw.Draw(self.image)\n",
    "        self.label.config(text=\"\")\n",
    "\n",
    "    def predict(self):\n",
    "        # Preprocess the image\n",
    "        img = self.image.resize((28, 28))\n",
    "        img = ImageOps.invert(img)\n",
    "        img = np.array(img).astype(np.float32) / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        # Display the prediction\n",
    "        self.label.config(text=f'Predicted Digit: {pred.item()}')\n",
    "\n",
    "# Run the application\n",
    "root = tk.Tk()\n",
    "app = DrawApp(root)\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
